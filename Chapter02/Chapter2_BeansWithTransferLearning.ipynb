{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Chapter2_BeansWithTransferLearning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPF6Tm1Xn_Px"
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\")\r\n",
        "# you may not use this file except in compliance with the License.\r\n",
        "# You may obtain a copy of the License at\r\n",
        "\r\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\r\n",
        "\r\n",
        "# Unless required by applicable law or agreed to in writing, software\r\n",
        "# distributed under the License is distributed on an \\\"AS IS\\\" BASIS,\r\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
        "# See the License for the specific language governing permissions and\r\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs6665391bst"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0cKXo_cW25V"
      },
      "source": [
        "# TensorFlow Hub is a repository of models\r\n",
        "# We'll pick one from there to get the already-learned features\r\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd0U-bRL1v5m"
      },
      "source": [
        "# This uses TFDS to load the beans dataset\n",
        "# I covered using TFDS in my book 'AI and Machine Learning for Programmers'\n",
        "(ds_train, ds_validation, ds_test), ds_info = tfds.load(\n",
        "    name = 'beans', \n",
        "    split = ['train', 'validation', 'test'],\n",
        "    as_supervised = True,\n",
        "    with_info = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAboigDC1yKV"
      },
      "source": [
        "# This continues setting up the data from TFDS to use in training the model\n",
        "batch_size=32\n",
        "def map_data(image, label, target_height = 224, target_width = 224):\n",
        "    \"\"\"Normalizes images: `unit8` -> `float32` and resizes images\n",
        "    by keeping the aspect ratio the same without distortion.\"\"\"\n",
        "    image = tf.cast(image, tf.float32)/255.\n",
        "    image = tf.image.resize_with_crop_or_pad(image, target_height, target_width)\n",
        "    return image, label\n",
        "\n",
        "ds_train = ds_train.map(map_data)\n",
        "ds_train = ds_train.cache()\n",
        "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
        "ds_train = ds_train.batch(batch_size)\n",
        "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "ds_validation = ds_validation.map(map_data)\n",
        "ds_validation = ds_validation.batch(batch_size)\n",
        "ds_validation = ds_validation.cache()\n",
        "ds_validation = ds_validation.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "ds_test = ds_test.map(map_data)\n",
        "ds_test = ds_test.batch(batch_size)\n",
        "ds_test = ds_test.cache()\n",
        "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXvozLrGW7VE"
      },
      "source": [
        "# You can experiment with different model types\n",
        "# The commented out one is a good model for crop diseases, as it was already trained on Cassava blight\n",
        "# The second one is MobileNet, a common one for mobile applications\n",
        "#model_handle = \"https://tfhub.dev/google/cropnet/feature_vector/cassava_disease_V1/1\"\n",
        "model_handle = \"https://tfhub.dev/google/imagenet/mobilenet_v2_035_224/feature_vector/4\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_L0hExwXJko"
      },
      "source": [
        "# HEre is where we take the model from hub, treat it as a layer called 'Feature Vector'\n",
        "# and add our own model beneath\n",
        "feature_vector = hub.KerasLayer(model_handle, trainable=False,\n",
        "                               input_shape=(224, 224, 3))\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  feature_vector,\n",
        "  tf.keras.layers.Dense(3, activation = 'softmax'),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IL-2eNlj9bp"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGjV_UIx2rr0"
      },
      "source": [
        "# Here is where we define the parameters to use when training the model\n",
        "# The loss function and the optimizer control how it learns\n",
        "model.compile(\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    optimizer = tf.keras.optimizers.Adam(.001),\n",
        "    metrics = ['accuracy'],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTevBM1b2uCW"
      },
      "source": [
        "# We can then train the model. 20 epochs with GPU in colab takes less than a minute!\n",
        "num_epochs = 20\n",
        "history = model.fit(\n",
        "    ds_train,\n",
        "    epochs = num_epochs,\n",
        "    validation_data = ds_validation,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L03ga_j5KGHI"
      },
      "source": [
        "# We can save the model to later convert it to JS or TFLite\r\n",
        "tf.saved_model.save(model, '/tmp/saved_model/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA9i5VP12xHt"
      },
      "source": [
        "# Here we can plot the loss and accuracy of the model as it trained\n",
        "# Accuracy should go up over time, loss should go down\n",
        "# THere are 2 curves -- for the training data, which the model used to 'figure out' how to fit images to labels\n",
        "# And for validation data, which wasn't used in the fitting, but can be a nice measurement of how accurate the model is\n",
        "# on data that it hadn't previously seen. In a good model, these curves will end up very close to each other\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs_range = range(num_epochs)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "# This model shows that the accuracy begins to diverge at about 15 epochs\n",
        "# which is a sign of overfitting, so it's not a great model.\n",
        "# But it's good enough for now! "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn3MTN5D4JiN"
      },
      "source": [
        "# That overfitting is borne out in the final test accuracy, \n",
        "# which is about 89%, where the model accuracy was about 92%\n",
        "# This shows that the model does better on data that it has 'seen' while\n",
        "# training, but not so well on data it hasn't yet seen. \n",
        "\n",
        "test_loss, test_acc = model.evaluate(ds_test)\n",
        "print('n Final test accuracy:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAdhU-beKPv1"
      },
      "source": [
        "def return_class_labels(ds):\n",
        "    \"\"\"\"Returns a list of class labels from a `DatasetV1Adapter` object.\"\"\"\n",
        "    l_labels = []\n",
        "    for _, labels in ds.take(-1):\n",
        "        labels = labels.numpy()\n",
        "        l_labels.append(labels[:])\n",
        "    return [item for sublist in l_labels for item in sublist]\n",
        "\n",
        "def get_text_label(labelval):\n",
        "  labels = {\n",
        "      0: \"Angular Leaf Spot\",\n",
        "      1: \"Leaf Rust\",\n",
        "      2: \"Healthy\"\n",
        "  }\n",
        "  return labels.get(labelval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICAFae9D4SPk"
      },
      "source": [
        "# This code will plot out a bunch of images, telling us the actual label (the diagnosed disease)\n",
        "# and the predicted label (what the model thinks the disease is)\n",
        "probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
        "normalized_probs = probability_model.predict(ds_test)\n",
        "predicted_labels = np.argmax(normalized_probs, axis = 1)\n",
        "actual_labels = return_class_labels(ds_test)\n",
        "\n",
        "# Looking at test images\n",
        "example = ds_test.take(1)\n",
        "for sample in example:\n",
        "    image = sample[0]\n",
        "    image = image.numpy()\n",
        "\n",
        "n_cols, n_rows = 4, 4\n",
        "plt.rcParams['figure.figsize'] = [n_cols*8, n_rows*8]\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(1, n_cols*n_rows + 1):\n",
        "    ax = fig.add_subplot(n_rows, n_cols,i)\n",
        "    ax.text(5, -9, \"actual: \" + get_text_label(actual_labels[i]) + \", predicted: \" + get_text_label(predicted_labels[i]) ,\n",
        "            color = 'red', fontsize = 15)\n",
        "    ax.imshow(image[i, :, :, :], cmap = plt.get_cmap(\"jet\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}